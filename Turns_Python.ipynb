{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3712d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 1. Import libraries\n",
    "# ------------------------------------------------------------\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Load data (use relative path & CSV file)\n",
    "# ------------------------------------------------------------\n",
    "# Make sure your CSV file is located in a subfolder called \"data\"\n",
    "# (create it if necessary and place CS_CDS.csv inside)\n",
    "FILE_DATA = \"data/CS_CDS.csv\"\n",
    "\n",
    "# Read the CSV file\n",
    "turns = pd.read_csv(FILE_DATA)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Display only the beginning and end of the DataFrame\n",
    "# ------------------------------------------------------------\n",
    "print(\"Data preview (first 6 rows):\")\n",
    "print(turns.head(6))\n",
    "print(\"\\nData preview (last 6 rows):\")\n",
    "print(turns.tail(6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e42115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 4. Number of dialogues by age\n",
    "# ------------------------------------------------------------\n",
    "# Check that required columns exist\n",
    "required_cols = {\"Age\", \"Dialogue/Interaction_unit\"}\n",
    "if not required_cols.issubset(turns.columns):\n",
    "    raise ValueError(f\"Missing one of the required columns: {required_cols}\")\n",
    "\n",
    "# Define the ages you want to inspect\n",
    "ages_to_check = [6, 12, 18]\n",
    "\n",
    "print(\"Maximum number of dialogues per age:\")\n",
    "for age in ages_to_check:\n",
    "    max_dialogue = turns.loc[turns[\"Age\"] == age, \"Dialogue/Interaction_unit\"].max()\n",
    "    print(f\" Age {age} months â†’ {max_dialogue}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b5c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 5. Average number of turns within dialogues by age\n",
    "# ------------------------------------------------------------\n",
    "import statistics\n",
    "\n",
    "# Ensure required columns exist\n",
    "required_cols = {\"Age\", \"Dialogue/Interaction_unit\", \"Number\"}\n",
    "if not required_cols.issubset(turns.columns):\n",
    "    raise ValueError(f\"Missing one of the required columns: {required_cols}\")\n",
    "\n",
    "ages_to_check = [6, 12, 18]\n",
    "\n",
    "print(\"Average number of turns per dialogue (by age):\")\n",
    "for age in ages_to_check:\n",
    "    subset = turns[turns[\"Age\"] == age]\n",
    "    if subset.empty:\n",
    "        print(f\" Age {age} months â†’ no data available\")\n",
    "        continue\n",
    "    \n",
    "    # Group by dialogue unit and get max turns per dialogue, then average across dialogues\n",
    "    avg_turns = statistics.mean(subset.groupby(\"Dialogue/Interaction_unit\")[\"Number\"].max())\n",
    "    print(f\" Age {age} months â†’ {avg_turns:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfde9fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 6. Distribution of languages by age\n",
    "# ------------------------------------------------------------\n",
    "# Check that 'Language' and 'Age' columns exist\n",
    "required_cols = {\"Language\", \"Age\"}\n",
    "if not required_cols.issubset(turns.columns):\n",
    "    raise ValueError(f\"Missing one of the required columns: {required_cols}\")\n",
    "\n",
    "ages_to_check = [6, 12, 18]\n",
    "\n",
    "print(\"Language distributions by age:\")\n",
    "for age in ages_to_check:\n",
    "    subset = turns[turns[\"Age\"] == age]\n",
    "    if subset.empty:\n",
    "        print(f\"\\nAge {age} months â†’ no data available\")\n",
    "        continue\n",
    "    print(f\"\\nAge {age} months:\")\n",
    "    print(subset[\"Language\"].value_counts(normalize=False))  # set True for proportions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f3aee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 7. Distribution of utterance types by age\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Check required columns\n",
    "required_cols = {\"Utterance_type\", \"Age\"}\n",
    "if not required_cols.issubset(turns.columns):\n",
    "    raise ValueError(f\"Missing one of the required columns: {required_cols}\")\n",
    "\n",
    "ages_to_check = [6, 12, 18]\n",
    "\n",
    "print(\"Utterance type distributions by age:\")\n",
    "for age in ages_to_check:\n",
    "    subset = turns[turns[\"Age\"] == age]\n",
    "    if subset.empty:\n",
    "        print(f\"\\nAge {age} months â†’ no data available\")\n",
    "        continue\n",
    "    print(f\"\\nAge {age} months:\")\n",
    "    print(subset[\"Utterance_type\"].value_counts(normalize=False))  # use normalize=True for proportions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d3207e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 8. Distribution of participants by age\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Check that necessary columns exist\n",
    "required_cols = {\"Participant\", \"Age\"}\n",
    "if not required_cols.issubset(turns.columns):\n",
    "    raise ValueError(f\"Missing one of the required columns: {required_cols}\")\n",
    "\n",
    "ages_to_check = [6, 12, 18]\n",
    "\n",
    "print(\"Participant distributions by age:\")\n",
    "for age in ages_to_check:\n",
    "    subset = turns[turns[\"Age\"] == age]\n",
    "    if subset.empty:\n",
    "        print(f\"\\nAge {age} months â†’ no data available\")\n",
    "        continue\n",
    "    print(f\"\\nAge {age} months:\")\n",
    "    print(subset[\"Participant\"].value_counts(normalize=False))  # normalize=True for proportions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61952a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 9. Turn initiations by child vs. CDS, per age\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "required_cols = {\"Age\", \"Participant\", \"Initiation\", \"CDS\"}\n",
    "if not required_cols.issubset(turns.columns):\n",
    "    raise ValueError(f\"Missing one of the required columns: {required_cols}\")\n",
    "\n",
    "ages_to_check = [6, 12, 18]\n",
    "\n",
    "for age in ages_to_check:\n",
    "    subset = turns[turns[\"Age\"] == age]\n",
    "    if subset.empty:\n",
    "        print(f\"Age {age} months â†’ no data available\")\n",
    "        continue\n",
    "\n",
    "    # child-initiated turns (participant == 'TCH')\n",
    "    init_cs = len(subset[(subset[\"Participant\"] == \"TCH\") & (subset[\"Initiation\"] == 1)])\n",
    "\n",
    "    # caregiver-initiated turns (CDS row + initiation)\n",
    "    init_cds = len(subset[(subset[\"CDS\"] == 1) & (subset[\"Initiation\"] == 1)])\n",
    "\n",
    "    # use total number of utterances at that age as denominator\n",
    "    total_utts = len(subset)\n",
    "\n",
    "    share_cs = init_cs / total_utts if total_utts else np.nan\n",
    "    share_cds = init_cds / total_utts if total_utts else np.nan\n",
    "\n",
    "    print(f\"\\nAge {age} months:\")\n",
    "    print(f\"  Child-initiated (TCH): {init_cs} / {total_utts} = {share_cs:.3f}\")\n",
    "    print(f\"  CDS-initiated:        {init_cds} / {total_utts} = {share_cds:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68107801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Load data using a relative path\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# define relative path (from current notebook location)\n",
    "file_path = Path(\"../data/Final.csv\")  # adjust \"../\" if needed\n",
    "\n",
    "# Read CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(f\"\\nâœ… Loaded {len(df):,} rows from {file_path}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Data cleaning\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Replace missing values in 'Duration' with 0\n",
    "df[\"Duration\"] = pd.to_numeric(df[\"Duration\"], errors=\"coerce\").fillna(0)\n",
    "\n",
    "# Drop empty rows\n",
    "df = df.dropna(how=\"all\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Descriptive summaries\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# 3.1 Total duration by speaker\n",
    "if \"Speaker\" in df.columns:\n",
    "    duration_by_speaker = df.groupby(\"Speaker\")[\"Duration\"].sum().sort_values(ascending=False)\n",
    "    print(\"\\nðŸŽ™ï¸ Total Duration by Speaker (s):\")\n",
    "    print(duration_by_speaker.head(10))  # show top 10\n",
    "    print(\"...\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Column 'Speaker' not found.\")\n",
    "\n",
    "# 3.2 Count of utterances by classification\n",
    "if \"Classification\" in df.columns:\n",
    "    utterance_count = df[\"Classification\"].value_counts()\n",
    "    print(\"\\nðŸ’¬ Utterance Count by Classification:\")\n",
    "    print(utterance_count)\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Column 'Classification' not found.\")\n",
    "\n",
    "# 3.3 Average duration by category\n",
    "if \"Category\" in df.columns:\n",
    "    avg_duration_category = df.groupby(\"Category\")[\"Duration\"].mean().sort_values(ascending=False)\n",
    "    print(\"\\nðŸ“Š Average Duration by Category (s):\")\n",
    "    print(avg_duration_category)\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Column 'Category' not found.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. Save cleaned version (optional)\n",
    "# ------------------------------------------------------------\n",
    "out_path = Path(\"../data/Final_cleaned.csv\")\n",
    "df.to_csv(out_path, index=False)\n",
    "print(f\"\\nâœ… Cleaned data saved to: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c61f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 9. Proportion of 'initiation' turns between 'tch' and 'cds'\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Filter only initiation turns\n",
    "if \"Turn\" not in df.columns or \"Category\" not in df.columns:\n",
    "    raise KeyError(\"Missing required columns: 'Turn' and/or 'Category'\")\n",
    "\n",
    "initiation_df = df[df[\"Turn\"].str.lower() == \"initiation\"]\n",
    "\n",
    "# Count how many initiations were produced by each category\n",
    "initiation_counts = initiation_df[\"Category\"].value_counts()\n",
    "\n",
    "# Extract counts safely (0 if not found)\n",
    "tch_count = initiation_counts.get(\"tch\", 0)\n",
    "cds_count = initiation_counts.get(\"cds\", 0)\n",
    "total = tch_count + cds_count\n",
    "\n",
    "# Calculate and display percentages\n",
    "if total > 0:\n",
    "    tch_percent = tch_count / total * 100\n",
    "    cds_percent = cds_count / total * 100\n",
    "\n",
    "    print(f\"\\nðŸŽ¯ Dialogue Initiation by Category (n = {total}):\")\n",
    "    print(f\"  tch: {tch_count:>4}  ({tch_percent:5.1f}%)\")\n",
    "    print(f\"  cds: {cds_count:>4}  ({cds_percent:5.1f}%)\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ No initiation data found for 'tch' or 'cds'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2831dfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Load data (relative CSV path instead of absolute XLSX)\n",
    "# ------------------------------------------------------------\n",
    "data_path = Path(\"../data/CDS_data.csv\")   # Adjust folder as needed\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Filter: keep only CDS rows, and split dialogue vs non-dialogue\n",
    "# ------------------------------------------------------------\n",
    "cds_df = df[df[\"Category\"].str.lower().eq(\"cds\")].copy()\n",
    "\n",
    "dialogue_df = cds_df[cds_df[\"Turn\"].isin([\"initiation\", \"contingent\"])].copy()\n",
    "non_dialogue_df = cds_df[cds_df[\"Turn\"].str.lower().eq(\"non-contingent\")].copy()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Total CDS utterances = denominator for percentages\n",
    "# ------------------------------------------------------------\n",
    "total_utterances = cds_df.shape[0]\n",
    "if total_utterances == 0:\n",
    "    raise ValueError(\"cds_df is empty â€” cannot compute percentages of total utterances.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. Clean up classification strings\n",
    "# ------------------------------------------------------------\n",
    "for _df in (dialogue_df, non_dialogue_df):\n",
    "    if \"Classification\" in _df.columns:\n",
    "        _df[\"Classification\"] = _df[\"Classification\"].astype(str).str.strip()\n",
    "    else:\n",
    "        raise KeyError(\"Missing column 'Classification' in dataset.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5. Compute absolute counts\n",
    "# ------------------------------------------------------------\n",
    "dialogue_counts = dialogue_df[\"Classification\"].value_counts()\n",
    "non_dialogue_counts = non_dialogue_df[\"Classification\"].value_counts()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6. Convert counts to % of total CDS utterances\n",
    "# ------------------------------------------------------------\n",
    "dialogue_pct_total = (dialogue_counts / total_utterances * 100).round(2)\n",
    "non_dialogue_pct_total = (non_dialogue_counts / total_utterances * 100).round(2)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7. Combine into a single tidy DataFrame\n",
    "# ------------------------------------------------------------\n",
    "all_classes = dialogue_pct_total.index.union(non_dialogue_pct_total.index)\n",
    "\n",
    "percent_total_df = pd.DataFrame({\n",
    "    \"Dialogue % (of total)\": dialogue_pct_total.reindex(all_classes, fill_value=0),\n",
    "    \"Non-Contingent % (of total)\": non_dialogue_pct_total.reindex(all_classes, fill_value=0)\n",
    "})\n",
    "\n",
    "percent_total_df[\"% Difference (Dialogue - Non)\"] = (\n",
    "    percent_total_df[\"Dialogue % (of total)\"] -\n",
    "    percent_total_df[\"Non-Contingent % (of total)\"]\n",
    ").round(2)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 8. Show short, GitHub-friendly output (no full table)\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nðŸ“Š Classification percentages relative to total CDS utterances:\")\n",
    "display(percent_total_df.head(6))\n",
    "display(percent_total_df.tail(6))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 9. Optional: save the summary\n",
    "# ------------------------------------------------------------\n",
    "out_path = Path(\"../results/classification_percentages.csv\")\n",
    "out_path.parent.mkdir(exist_ok=True)\n",
    "percent_total_df.to_csv(out_path)\n",
    "print(f\"\\nâœ… Summary saved to: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223a00ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) basic counts -------------------------------------------------\n",
    "total_utterances = len(df)\n",
    "tch_mask = df[\"Speaker\"].astype(str).str.lower().eq(\"tch\")\n",
    "tch_utterances = df[tch_mask].shape[0]\n",
    "\n",
    "tch_percentage = (tch_utterances / total_utterances * 100) if total_utterances else 0\n",
    "\n",
    "print(f\"Total utterances: {total_utterances}\")\n",
    "print(f\"Utterances by 'tch': {tch_utterances} ({tch_percentage:.2f}%)\")\n",
    "\n",
    "# 2) define dialogue vs non-contingent ---------------------------\n",
    "# make sure 'Turn' exists and is string\n",
    "if \"Turn\" not in df.columns:\n",
    "    raise KeyError(\"Column 'Turn' not found in dataframe.\")\n",
    "\n",
    "df[\"Turn\"] = df[\"Turn\"].astype(str).str.lower()\n",
    "\n",
    "df[\"Dialogue_Status\"] = df[\"Turn\"].apply(\n",
    "    lambda x: \"Dialogue\" if x in [\"initiation\", \"contingent\"] else \"Non-Contingent\"\n",
    ")\n",
    "\n",
    "# 3) define speaker group (tch vs. everyone else) ----------------\n",
    "df[\"Speaker\"] = df[\"Speaker\"].astype(str)\n",
    "df[\"Speaker_Group\"] = df[\"Speaker\"].str.lower().apply(\n",
    "    lambda x: \"tch\" if x == \"tch\" else \"non-tch\"\n",
    ")\n",
    "\n",
    "# 4) cross-tab: speaker group Ã— dialogue status ------------------\n",
    "speaker_dialogue_counts = (\n",
    "    df.groupby([\"Speaker_Group\", \"Dialogue_Status\"])\n",
    "      .size()\n",
    "      .unstack(fill_value=0)\n",
    ")\n",
    "\n",
    "# add row totals\n",
    "speaker_dialogue_counts[\"Total\"] = speaker_dialogue_counts.sum(axis=1)\n",
    "\n",
    "print(\"\\nUtterance counts by speaker group and dialogue status:\")\n",
    "print(speaker_dialogue_counts.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4361b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example: df['Turn'] = ['initiation', 'contingent', 'contingent', 'non-contingent', 'initiation', 'contingent', ...]\n",
    "\n",
    "# Step 1: Initialize variables\n",
    "dialogical_units = []  # stores list of dialogical unit lengths\n",
    "current_unit = 0\n",
    "inside_unit = False\n",
    "\n",
    "# Step 2: Iterate through the Turn column\n",
    "for turn in df['Turn']:\n",
    "    if turn == 'initiation':\n",
    "        if inside_unit:\n",
    "            dialogical_units.append(current_unit)\n",
    "        current_unit = 1\n",
    "        inside_unit = True\n",
    "    elif inside_unit and turn == 'contingent':\n",
    "        current_unit += 1\n",
    "    elif inside_unit and turn == 'non-contingent':\n",
    "        dialogical_units.append(current_unit)\n",
    "        current_unit = 0\n",
    "        inside_unit = False\n",
    "    else:\n",
    "        continue  # Skip other cases if needed\n",
    "\n",
    "# Handle final dialogical unit (if it ends the file without closing)\n",
    "if inside_unit and current_unit > 0:\n",
    "    dialogical_units.append(current_unit)\n",
    "\n",
    "# Step 3: Calculate average\n",
    "if dialogical_units:\n",
    "    average_turns = sum(dialogical_units) / len(dialogical_units)\n",
    "    print(f\"Average number of turns per dialogical unit: {average_turns:.2f}\")\n",
    "else:\n",
    "    print(\"No dialogical units found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff591cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fisher's Exact Test\n",
    "import pandas as pd\n",
    "from scipy.stats import fisher_exact\n",
    "\n",
    "# assume cds_df already exists and is filtered to Category == 'cds'\n",
    "df = cds_df.copy()\n",
    "\n",
    "# 1) clean strings ------------------------------------------------\n",
    "df[\"Classification\"] = df[\"Classification\"].astype(str).str.strip().str.lower()\n",
    "df[\"Turn\"] = df[\"Turn\"].astype(str).str.lower()\n",
    "\n",
    "# 2) merge fine-grained codes into broader ones -------------------\n",
    "merge_map = {\n",
    "    \"vocal imitation\": \"imitation\",\n",
    "    \"iconic imitation\": \"imitation\",\n",
    "    \"prosodic imitation\": \"imitation\",\n",
    "    \"semantic expansion\": \"expansion\",\n",
    "    \"syntactic expansion\": \"expansion\",\n",
    "    \"repair 1\": \"repair\",\n",
    "    \"repair 2\": \"repair\",\n",
    "    \"repair 3\": \"repair\",\n",
    "}\n",
    "df[\"Classification\"] = df[\"Classification\"].replace(merge_map)\n",
    "\n",
    "# 3) define \"in turn\" vs. \"non-contingent\" ------------------------\n",
    "df[\"is_in_turn\"] = df[\"Turn\"].isin([\"contingent\", \"initiation\"])\n",
    "\n",
    "results = []\n",
    "\n",
    "# 4) loop over utterance classes ----------------------------------\n",
    "for cls in df[\"Classification\"].dropna().unique():\n",
    "    is_cls = df[\"Classification\"] == cls\n",
    "    is_other = ~is_cls\n",
    "\n",
    "    a = (is_cls & df[\"is_in_turn\"]).sum()          # this class, in turn\n",
    "    b = (is_cls & ~df[\"is_in_turn\"]).sum()         # this class, out of turn\n",
    "    c = (is_other & df[\"is_in_turn\"]).sum()        # all other classes, in turn\n",
    "    d = (is_other & ~df[\"is_in_turn\"]).sum()       # all other classes, out of turn\n",
    "\n",
    "    table = [[a, b],\n",
    "             [c, d]]\n",
    "\n",
    "    # fisher_exact needs a valid 2x2 with nonnegative ints\n",
    "    if (a + b > 0) and (c + d > 0):\n",
    "        oddsratio, p = fisher_exact(table)\n",
    "    else:\n",
    "        oddsratio, p = float(\"nan\"), float(\"nan\")\n",
    "\n",
    "    results.append({\n",
    "        \"utterance_type\": cls,\n",
    "        \"in_turn\": a,\n",
    "        \"non_contingent\": b,\n",
    "        \"odds_ratio\": oddsratio,\n",
    "        \"p_value\": p,\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(\"p_value\")\n",
    "\n",
    "# show only the interesting ones, as per github comments\n",
    "sig_df = results_df[results_df[\"p_value\"] < 0.05]\n",
    "\n",
    "print(\"\\nSignificant associations (Fisherâ€™s exact, p < .05):\")\n",
    "print(sig_df.head(10))         # donâ€™t dump the whole thing\n",
    "print(\"\\nAll results (head):\")\n",
    "print(results_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6bd755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian Comparison of Two Proportions\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import beta\n",
    "\n",
    "df = cds_df.copy()\n",
    "df['Classification'] = df['Classification'].str.strip()\n",
    "# Merge utterance types into broader categories\n",
    "df['Classification'] = df['Classification'].replace({\n",
    "    'vocal imitation': 'imitation',\n",
    "    'iconic imitation': 'imitation',\n",
    "    'prosodic imitation': 'imitation',\n",
    "    'semantic expansion': 'expansion',\n",
    "    'syntactic expansion': 'expansion',\n",
    "    'repair 1': 'repair',\n",
    "    'repair 2': 'repair',\n",
    "    'repair 3': 'repair'\n",
    "})\n",
    "\n",
    "# Create binary column for turn-taking\n",
    "df['is_in_turn'] = df['Turn'].isin(['contingent', 'initiation'])\n",
    "\n",
    "# Define the classification of interest\n",
    "target_class = 'Imperative'  # â† change this to whatever category you're analyzing\n",
    "\n",
    "# Filter for each group\n",
    "in_turn = df[df['is_in_turn'] == 'True']\n",
    "non_contingent = df[df['is_in_turn'] == 'False']\n",
    "\n",
    "# Count how many imperatives in each group (successes)\n",
    "k1 = (in_turn['Classification'] == target_class).sum()\n",
    "k2 = (non_contingent['Classification'] == target_class).sum()\n",
    "\n",
    "# Total utterances in each group (sample size)\n",
    "n1 = len(in_turn)\n",
    "n2 = len(non_contingent)\n",
    "\n",
    "# Posterior distributions assuming uniform Beta(1,1) priors\n",
    "posterior1 = beta(a=1 + k1, b=1 + n1 - k1)\n",
    "posterior2 = beta(a=1 + k2, b=1 + n2 - k2)\n",
    "\n",
    "# Sample from posteriors\n",
    "samples1 = posterior1.rvs(10000)\n",
    "samples2 = posterior2.rvs(10000)\n",
    "diff_samples = samples1 - samples2\n",
    "\n",
    "# Summary statistics\n",
    "mean_diff = np.mean(diff_samples)\n",
    "cred_interval = np.percentile(diff_samples, [2.5, 97.5])\n",
    "prob_diff_gt_0 = np.mean(diff_samples > 0)\n",
    "\n",
    "print(f\"Mean difference: {mean_diff:.3f}\")\n",
    "print(f\"95% credible interval: {cred_interval}\")\n",
    "print(f\"Probability In-Turn > Non-Contingent: {prob_diff_gt_0:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daba9903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import beta\n",
    "\n",
    "# work on a copy\n",
    "df = cds_df.copy()\n",
    "\n",
    "# 1) clean text columns -------------------------------------------------\n",
    "df[\"Classification\"] = (\n",
    "    df[\"Classification\"]\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    ")\n",
    "df[\"Turn\"] = df[\"Turn\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "# 2) merge fine-grained labels -----------------------------------------\n",
    "merge_map = {\n",
    "    \"vocal imitation\": \"imitation\",\n",
    "    \"iconic imitation\": \"imitation\",\n",
    "    \"prosodic imitation\": \"imitation\",\n",
    "    \"semantic expansion\": \"expansion\",\n",
    "    \"syntactic expansion\": \"expansion\",\n",
    "    \"repair 1\": \"repair\",\n",
    "    \"repair 2\": \"repair\",\n",
    "    \"repair 3\": \"repair\",\n",
    "}\n",
    "df[\"Classification\"] = df[\"Classification\"].replace(merge_map)\n",
    "\n",
    "# 3) define turn-taking vs. non-contingent ------------------------------\n",
    "df[\"is_in_turn\"] = df[\"Turn\"].isin([\"contingent\", \"initiation\"])\n",
    "\n",
    "# 4) prep containers ----------------------------------------------------\n",
    "classifications = df[\"Classification\"].dropna().unique()\n",
    "results = []\n",
    "\n",
    "# 5) loop over all classes ----------------------------------------------\n",
    "for cls in classifications:\n",
    "    # split once per iteration\n",
    "    in_turn = df[df[\"is_in_turn\"]]\n",
    "    non_turn = df[~df[\"is_in_turn\"]]\n",
    "\n",
    "    # counts for this class\n",
    "    k1 = (in_turn[\"Classification\"] == cls).sum()\n",
    "    k2 = (non_turn[\"Classification\"] == cls).sum()\n",
    "    n1 = len(in_turn)\n",
    "    n2 = len(non_turn)\n",
    "\n",
    "    # Beta(1,1) priors\n",
    "    post1 = beta(a=1 + k1, b=1 + (n1 - k1))\n",
    "    post2 = beta(a=1 + k2, b=1 + (n2 - k2))\n",
    "\n",
    "    # sample\n",
    "    samples1 = post1.rvs(10_000)\n",
    "    samples2 = post2.rvs(10_000)\n",
    "    diff = samples1 - samples2\n",
    "\n",
    "    mean_diff = diff.mean()\n",
    "    ci_low, ci_high = np.percentile(diff, [2.5, 97.5])\n",
    "    prob_gt0 = (diff > 0).mean()\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            \"classification\": cls,\n",
    "            \"mean_diff_inturn_minus_non\": round(mean_diff, 3),\n",
    "            \"ci_2.5\": round(ci_low, 3),\n",
    "            \"ci_97.5\": round(ci_high, 3),\n",
    "            \"p(in_turn > non_turn)\": round(prob_gt0, 3),\n",
    "            \"in_turn_count\": int(k1),\n",
    "            \"non_turn_count\": int(k2),\n",
    "        }\n",
    "    )\n",
    "\n",
    "# 6) to DataFrame and sort ----------------------------------------------\n",
    "results_df = (\n",
    "    pd.DataFrame(results)\n",
    "    .sort_values(by=\"p(in_turn > non_turn)\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# show only top rows so the notebook/github doesnâ€™t explode\n",
    "print(\"\\nBayesian comparison of CDS classifications (top 15):\")\n",
    "print(results_df.head(15))\n",
    "\n",
    "# if you want to save:\n",
    "# results_df.to_csv(\"../results/bayesian_turn_comparison.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7af923",
   "metadata": {},
   "source": [
    "### CHARTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa9a3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 1. Input: coefficients + SE from the R hglm output\n",
    "# -------------------------------------------------------\n",
    "# R output:\n",
    "# avg.CDS       -1.065e-02  SE = 5.778e-03  p = 0.0736\n",
    "# avg.CTC        1.142e-02  SE = 1.094e-03  p = 1.92e-12\n",
    "# SES            6.735e-02  SE = 6.294e-03  p = 9.90e-13\n",
    "# avg.CTC:SES   -1.584e-04  SE = 2.013e-05  p = 2.45e-09\n",
    "\n",
    "data = {\n",
    "    \"label\":   [\"CDS\", \"CTC\", \"SES\", \"CTCÃ—SES\"],\n",
    "    \"coef\":    [-1.065e-02,  1.142e-02,  6.735e-02, -1.584e-04],\n",
    "    \"se\":      [ 5.778e-03,  1.094e-03,  6.294e-03,  2.013e-05],\n",
    "    \"p\":       [ 7.36e-02,   1.92e-12,   9.90e-13,   2.45e-09],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df[\"sig\"] = df[\"p\"] < 0.05\n",
    "\n",
    "# 95% CI on the log scale\n",
    "z = 1.96\n",
    "df[\"lower_coef\"] = df[\"coef\"] - z * df[\"se\"]\n",
    "df[\"upper_coef\"] = df[\"coef\"] + z * df[\"se\"]\n",
    "\n",
    "# convert to IRR\n",
    "df[\"irr\"] = np.exp(df[\"coef\"])\n",
    "df[\"lower_irr\"] = np.exp(df[\"lower_coef\"])\n",
    "df[\"upper_irr\"] = np.exp(df[\"upper_coef\"])\n",
    "\n",
    "# sort so SES etc. are nicely ordered (top-to-bottom)\n",
    "df = df.sort_values(\"irr\", ascending=True).reset_index(drop=True)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2. Global style\n",
    "# -------------------------------------------------------\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 14,\n",
    "    \"axes.titlesize\": 18,\n",
    "    \"axes.labelsize\": 16,\n",
    "    \"xtick.labelsize\": 14,\n",
    "    \"ytick.labelsize\": 14,\n",
    "    \"legend.fontsize\": 14,\n",
    "})\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3. Dot-and-whisker plot (horizontal)\n",
    "# -------------------------------------------------------\n",
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "\n",
    "y_pos = np.arange(len(df))\n",
    "\n",
    "# whiskers\n",
    "for i, row in df.iterrows():\n",
    "    ax.hlines(y=i,\n",
    "              xmin=row[\"lower_irr\"],\n",
    "              xmax=row[\"upper_irr\"],\n",
    "              color=\"gray\",\n",
    "              linewidth=2)\n",
    "\n",
    "# points\n",
    "colors = [\"steelblue\" if sig else \"lightgray\" for sig in df[\"sig\"]]\n",
    "ax.scatter(df[\"irr\"], y_pos, color=colors, s=80, zorder=3, edgecolor=\"black\")\n",
    "\n",
    "# reference line at IRR = 1\n",
    "ax.axvline(1, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(df[\"label\"])\n",
    "ax.set_xlabel(\"Incident rate ratio (exp(coef))\")\n",
    "ax.set_title(\"Predictors of 18-month vocabulary (Poisson hGLM)\")\n",
    "\n",
    "# legend for significance\n",
    "legend_elems = [\n",
    "    Patch(facecolor=\"steelblue\", edgecolor=\"black\", label=\"p < .05\"),\n",
    "    Patch(facecolor=\"lightgray\", edgecolor=\"black\", label=\"n.s.\")\n",
    "]\n",
    "ax.legend(handles=legend_elems, loc=\"lower right\", frameon=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 4. Save as PDF (vector)\n",
    "# -------------------------------------------------------\n",
    "plt.savefig(r\"C:\\Users\\lvfeu\\Desktop\\Plot_coefficients.pdf\",\n",
    "            format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54163150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib.font_manager import FontProperties\n",
    "personal_path = 'C:/Users/lvfeu/Desktop/PhD/University of Zurich/Paper 2/Metaphor Identification Corpus 3 modalities/'\n",
    "\n",
    "# get normal font and store it in `Helvetica`\n",
    "font_path = personal_path + 'helveticaneue.ttf'\n",
    "Helvetica = FontProperties(fname=font_path)\n",
    "\n",
    "# -----------------------------\n",
    "# Data Setup\n",
    "# -----------------------------\n",
    "ages = ['6 m.o.', '12 m.o. (1)', '12 m.o. (2)', '18 m.o.']\n",
    "x_positions = [6, 12, 12, 18]\n",
    "y_positions = [300, 440, 160, 300]  # Stack 12 mo pies vertically\n",
    "total_dialogues = np.array([125, 515, 501, 336])\n",
    "child_pct = np.array([0.24, 0.53, 0.499, 0.634])\n",
    "caregiver_pct = 1 - child_pct\n",
    "colors = ['#377eb8', '#ff7f00']  # Blue for child, orange for caregiver\n",
    "\n",
    "# -----------------------------\n",
    "# Pie Size Scaling\n",
    "# -----------------------------\n",
    "max_radius = 2  # control how big the biggest pie is\n",
    "scaled_radii = np.sqrt(total_dialogues / total_dialogues.max()) * max_radius\n",
    "pie_diameters = scaled_radii * 2  # convert radius to diameter\n",
    "\n",
    "# -----------------------------\n",
    "# Plot Setup\n",
    "# -----------------------------\n",
    "fig, ax = plt.subplots(figsize=(15, 10), dpi=600)\n",
    "ax.set_xlim(5, 20)\n",
    "ax.set_ylim(0, 600)\n",
    "\n",
    "ax.set_xlabel(\"Child's Age (months)\", fontsize=24, font=Helvetica, labelpad=15)\n",
    "ax.set_ylabel('Turn initiation percentage in all dialogues', fontsize=26, font=Helvetica, labelpad=15)\n",
    "ax.set_title('Dialogue Initiation Across Ages', font=Helvetica, fontsize=30, pad=20)\n",
    "\n",
    "ax.tick_params(axis='y', which='both', left=False, labelleft=False)\n",
    "for label in ax.get_xticklabels():\n",
    "    label.set_fontproperties(Helvetica)\n",
    "ax.tick_params(axis='x', labelsize=20)\n",
    "\n",
    "# -----------------------------\n",
    "# Draw Pies + % Labels\n",
    "# -----------------------------\n",
    "for i in range(len(ages)):\n",
    "    center_x, center_y = x_positions[i], y_positions[i]\n",
    "    diameter = pie_diameters[i]\n",
    "\n",
    "    # Pie container\n",
    "    inset_ax = inset_axes(ax,\n",
    "                          width=diameter,\n",
    "                          height=diameter,\n",
    "                          loc='center',\n",
    "                          bbox_to_anchor=(center_x, center_y),\n",
    "                          bbox_transform=ax.transData,\n",
    "                          borderpad=0)\n",
    "\n",
    "    # Draw pie\n",
    "    wedges, _ = inset_ax.pie(\n",
    "        [child_pct[i], caregiver_pct[i]],\n",
    "        colors=colors,\n",
    "        startangle=90,\n",
    "        wedgeprops=dict(edgecolor='white')\n",
    "    )\n",
    "    inset_ax.set_aspect('equal')\n",
    "    inset_ax.axis('off')\n",
    "\n",
    "    # Add child % label inside the blue wedge\n",
    "    inset_ax.text(-0.4, 0.17, f\"{round(child_pct[i]*100)}%\",\n",
    "                  color='white', fontsize=25, weight='bold',\n",
    "                  ha='center', va='center', font=Helvetica)\n",
    "\n",
    "    # Add age label below each pie\n",
    "    ax.text(center_x, center_y - diameter * 37, ages[i], ha='center', fontsize=23, font=Helvetica)\n",
    "\n",
    "# -----------------------------\n",
    "# Legend\n",
    "# -----------------------------\n",
    "legend_labels = ['Child-initiated', 'Caregiver-initiated']\n",
    "legend_patches = [plt.Line2D([0], [0], marker='o', color='w',\n",
    "                              markerfacecolor=c, markersize=16)\n",
    "                  for c in colors]\n",
    "\n",
    "ax.legend(legend_patches, legend_labels,\n",
    "          loc='upper right', fontsize=26, frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "# Pad margins so that markers don't get clipped by the axes\n",
    "plt.margins(0.2)\n",
    "\n",
    "# plt.savefig(r\"C:\\Users\\lvfeu\\Desktop\\Initiation_Dialogues.jpg\", dpi=600, bbox_inches=\"tight\")\n",
    "plt.savefig(r\"C:\\Users\\lvfeu\\Desktop\\Plot_coefficients.pdf\",\n",
    "            format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb03c775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# Data Setup (example from image)\n",
    "# -----------------------------\n",
    "data = {\n",
    "    'Classification': [\n",
    "        'Description', 'Question', 'Backchannel', 'Imperative', 'Exclamation',\n",
    "        'Imitation', 'Scaffolding', 'Play', 'Expansion', 'Praise',\n",
    "        'Iconic', 'Repair', 'Singing'\n",
    "    ],\n",
    "    'Within_Dialogue': [9.39, 8.46, 6.39, 3.95, 3.23, 3.27, 2.96, 2.50, 1.82, 1.67, 1.20, 1, 0.45],\n",
    "    'Outside_Dialogue': [15.74, 10.19, 1.72, 4.32, 5.36, 0.04, 4.94, 5.19, 0.08, 2.11, 1.67, 0.02, 1.45]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.sort_values('Outside_Dialogue', ascending=True, inplace=True)\n",
    "y_pos = np.arange(len(df))\n",
    "\n",
    "# -----------------------------\n",
    "# Plot Setup\n",
    "# -----------------------------\n",
    "fig, ax = plt.subplots(figsize=(15, 10), dpi=600)\n",
    "\n",
    "# Draw lines between the two sides\n",
    "for i in range(len(df)):\n",
    "    ax.plot(\n",
    "        [-1 * df['Outside_Dialogue'].iloc[i], 1 * df['Within_Dialogue'].iloc[i]],\n",
    "        [y_pos[i], y_pos[i]],\n",
    "        color='gray', lw=2, zorder=1\n",
    "    )\n",
    "\n",
    "# Scatter plots (dots)\n",
    "ax.scatter(-1 * df['Outside_Dialogue'], y_pos, color='#ff7f00', s=200, label='Outside Dialogue', zorder=2)\n",
    "ax.scatter(1 * df['Within_Dialogue'], y_pos, color='#377eb8', s=200, label='Within Dialogue', zorder=2)\n",
    "\n",
    "# -----------------------------\n",
    "# Bold selective labels\n",
    "# -----------------------------\n",
    "highlight = {'Imitation', 'Expansion', 'Repair'}\n",
    "formatted_labels = [\n",
    "    f\"$\\\\bf{{{label}}}$\" if label in highlight else label\n",
    "    for label in df['Classification']\n",
    "]\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(formatted_labels, font=Helvetica, fontsize=24)\n",
    "\n",
    "# -----------------------------\n",
    "# Axis settings\n",
    "# -----------------------------\n",
    "ax.set_xlabel('Proportion (%)', font=Helvetica, fontsize=24)\n",
    "ax.set_title('Within vs Outside Dialogue: CDS Utterance Classification', font=Helvetica, fontsize=30, pad=15)\n",
    "\n",
    "ax.axvline(0, color='black', lw=1)\n",
    "ax.set_xlim(-20, 20)\n",
    "ax.set_xticks(np.arange(-20, 21, 5))\n",
    "ax.set_xticklabels([f\"{abs(t)}%\" for t in np.arange(-20, 21, 5)], font=Helvetica, fontsize=18)\n",
    "\n",
    "# -----------------------------\n",
    "# Legend styling\n",
    "# -----------------------------\n",
    "legend = ax.legend(loc='lower left')\n",
    "# Optional: apply custom font\n",
    "for text in legend.get_texts():\n",
    "    text.set_fontproperties(Helvetica)\n",
    "    text.set_fontsize(26)\n",
    "\n",
    "plt.grid(True, axis='x', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "plt.tight_layout()\n",
    "#plt.margins(0.2)\n",
    "\n",
    "plt.savefig(r\"C:\\Users\\lvfeu\\Desktop\\Within_Outside_Dialogues.pdf\", dpi=600)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7455b0b",
   "metadata": {},
   "source": [
    "### Reliabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752008b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import cohen_kappa_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# --- 1. load data (relative path) ---\n",
    "# put the file in e.g. \"data/Reliabilities_Test_OlgaS.xlsx\"\n",
    "file_path = Path(\"data\") / \"Reliabilities_Test_OlgaS.xlsx\"\n",
    "df = pd.read_excel(file_path, sheet_name=\"Test\")\n",
    "\n",
    "# --- 2. select the two annotator columns and drop rows where either is missing ---\n",
    "ann = df[[\"Classification 1\", \"Classification 2\"]].dropna(subset=[\"Classification 1\", \"Classification 2\"])\n",
    "\n",
    "# normalize (strip + lower)\n",
    "ann = ann.apply(lambda col: col.astype(str).str.strip().str.lower())\n",
    "\n",
    "# --- 3. Cohenâ€™s kappa ---\n",
    "kappa = cohen_kappa_score(ann[\"Classification 1\"], ann[\"Classification 2\"])\n",
    "print(f\"Cohen's kappa: {kappa:.2f}\")\n",
    "\n",
    "# --- 4. confusion matrix (to see which labels disagree) ---\n",
    "labels = sorted(set(ann[\"Classification 1\"]).union(ann[\"Classification 2\"]))\n",
    "cm = confusion_matrix(ann[\"Classification 1\"], ann[\"Classification 2\"], labels=labels)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot(ax=ax, cmap=\"Blues\", xticks_rotation=45, colorbar=False)\n",
    "ax.set_title(\"Coder agreement by label\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
